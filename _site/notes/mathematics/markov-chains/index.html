<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Understanding Markov Chains</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Understanding Markov Chains" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="So I was watching this video." />
<meta property="og:description" content="So I was watching this video." />
<link rel="canonical" href="http://localhost:4000/notes/mathematics/markov-chains/" />
<meta property="og:url" content="http://localhost:4000/notes/mathematics/markov-chains/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-27T01:35:18+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Understanding Markov Chains" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-27T01:35:18+01:00","datePublished":"2025-08-27T01:35:18+01:00","description":"So I was watching this video.","headline":"Understanding Markov Chains","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/notes/mathematics/markov-chains/"},"url":"http://localhost:4000/notes/mathematics/markov-chains/"}</script>
<!-- End Jekyll SEO tag -->

    <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global',
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

  </head>
  <body>
    <div class="main-container">
        <div class="navbar">
  
    <a href="/">
        HOME
    </a>
  
    <a href="/blog/">
        BLOG
    </a>
  
    <a href="/projects/">
        PROJECTS
    </a>
  
    <a href="/notes/">
        NOTES
    </a>
  
    <a href="/now/">
        NOW
    </a>
  
</div>

        <!-- <h1 class="page-title">Understanding Markov Chains</h1> -->
        <div class="content-container">
            <h1 class="page-title">Understanding Markov Chains</h1>
<p>So I was watching this <a href="https://www.youtube.com/watch?v=_d3z2wVtDD8">video</a>.</p>

<p>It went over the concept of Markov chains and bigrams to help create the generative
language model. Seemed like pretty cool topics and after looking them up a bit I am 
interested in working on a project using those concepts.</p>

<p>From what I understand, Markov chains are one of primary foundations of the modern 
language models.</p>

<h2 id="what-is-a-markov-chain">What is a Markov chain?</h2>
<p>A <strong>Markov chain</strong> is a stochastic process that satisfies the <strong>markov property</strong>:</p>
<blockquote>
  <p>The probability of the next state depending only on the current state, not on the 
full history of past states.</p>
</blockquote>

<p>Formally expressed as:</p>

\[P(X_{n+1} \mid X_n, X_{n-1}, \ldots) = P(X_{n+1} \mid X_n)\]

<p>This means that knowing the present state $X_n$ is enough to predict the next state 
$X_{n+1}$ as well as if you had access to the entire past.
This has to be conditional on both past and present values.</p>

<h2 id="what-is-a-bigram">What is a bigram?</h2>
<p>A <strong>bigram</strong> is a specific application of a first-order Markov chain in linguistics,
used to model sequences of words. It predicts the current word based solely on the
immediately preceding word.</p>

<p>Bigram:</p>

\[P(w_i \mid w_{i-1})\]

<p>There are also <strong>higher-order markov models</strong> such as trigrams:</p>

\[P(w_i \mid w_{i-2}, w_{i-1})\]

<h2 id="why-people-shifted-the-notation-from-traditional-markov-chain-to-n-order-markov-chain-like-a-bigram">Why people shifted the notation from traditional markov chain to n-order markov chain like a bigram</h2>

<p>The goal in NLP is to predict the next word in the sequence. The question that is 
asked is: <strong>“What is the probability of this word, given the words that came 
before”.</strong></p>

<p>This leads to the following notation being used:</p>

\[P(w_1 \mid w_{i-1})\]

<p>Where the previous words are treated as the known context (input) and the current 
word is the target being predicted.</p>

<p>In contrast, traditional markov chains follow the question: <strong>“Given the current
state, what is the probability of the next state.”</strong></p>

<p>It is just a matter of how they decided to frame the problem in their respective 
domains, which is why the notation differs.</p>

<h3 id="next-steps">NEXT STEPS</h3>
<ul>
  <li>look into the evolution of markov chains, <strong>hidden markov models(hmm)</strong></li>
  <li>think of a project to use markov chains. Very cool topic.</li>
  <li>Interesting articles to read:
    <ul>
      <li><a href="https://www.danieleteti.it/post/from-markov-chains-to-modern-llms/#best-practices-and-pitfalls">1</a></li>
      <li><a href="https://medium.com/data-science/markov-models-and-markov-chains-explained-in-real-life-probabilistic-workout-routine-65e47b5c9a73">2</a></li>
      <li><a href="https://web.stanford.edu/~jurafsky/slp3/">3</a></li>
    </ul>
  </li>
</ul>


        </div>
        <hr>
<footer>
    <div class="footer-container">
        <div class="footer-links-container"> 
            Find me here:
            <a href="mailto:hasanur@elpatatone.com">Email</a> |
            <a href="https://github.com/ElPatatone">Github</a> |
            <a href="https://www.linkedin.com/in/hasanur-rahman-mohammad">Linkedin</a> 
        </div>
        <div class="quote-container">
            <p><em>
                "Real understanding begins where the abstraction ends."
            </em></p>
        </div>
    </div>
</footer>

    </div>
  </body>
</html>

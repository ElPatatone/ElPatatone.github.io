<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Why does CUDA work the way it does?</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Why does CUDA work the way it does?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How does memory work in the GPU? A single bit of memory is a capacitor, either on or off. The memory is read by switching a transistor on which is connected to the wire, the wire will in turn carry a voltage based on the charge from the capacitor." />
<meta property="og:description" content="How does memory work in the GPU? A single bit of memory is a capacitor, either on or off. The memory is read by switching a transistor on which is connected to the wire, the wire will in turn carry a voltage based on the charge from the capacitor." />
<link rel="canonical" href="http://localhost:4000/notes/gpu-computing/why-does-cuda-work-the-way-it-does/" />
<meta property="og:url" content="http://localhost:4000/notes/gpu-computing/why-does-cuda-work-the-way-it-does/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-27T21:56:26+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Why does CUDA work the way it does?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-27T21:56:26+01:00","datePublished":"2025-08-27T21:56:26+01:00","description":"How does memory work in the GPU? A single bit of memory is a capacitor, either on or off. The memory is read by switching a transistor on which is connected to the wire, the wire will in turn carry a voltage based on the charge from the capacitor.","headline":"Why does CUDA work the way it does?","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/notes/gpu-computing/why-does-cuda-work-the-way-it-does/"},"url":"http://localhost:4000/notes/gpu-computing/why-does-cuda-work-the-way-it-does/"}</script>
<!-- End Jekyll SEO tag -->

    <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global',
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

  </head>
  <body>
    <div class="main-container">
        <div class="navbar">
  
    <a href="/">
        HOME
    </a>
  
    <a href="/blog/">
        BLOG
    </a>
  
    <a href="/projects/">
        PROJECTS
    </a>
  
    <a href="/notes/">
        NOTES
    </a>
  
    <a href="/now/">
        NOW
    </a>
  
</div>

        <!-- <h1 class="page-title">Why does CUDA work the way it does?</h1> -->
        <div class="content-container">
            <h1 class="page-title">Why does CUDA work the way it does?</h1>
<h2 id="how-does-memory-work-in-the-gpu">How does memory work in the GPU?</h2>
<p>A single bit of memory is a capacitor, either on or off. The memory is read by 
switching a transistor on which is connected to the wire, the wire will in turn carry
a voltage based on the charge from the capacitor.</p>

<p>The <strong>DRAM</strong> chip contains millions of this cells that are all connected together 
in a 2D matrix. You can access data in any of the rows and columns in this matrix 
giving the property of <strong>random access memory</strong>. The data has a row and column index.</p>

<p>[look up]<br />
DRAM and how it reads memory</p>

<ol>
  <li>pull data from the desired row into the <strong>sense amplifiers</strong>, the sense amplifiers
take the small charge from the capacitors in the memory cell and turn it into 
easier to read, more defined voltages for the next steps.
The capacitors are drained of their charge through this process so the data in the 
capacitors is deleted as a result.</li>
  <li>read from the data stored in the amplifiers at the desired column index, this
does not delete the data in the amplifiers. Reading data from the amplifiers is also
much quicker and easier as the amplifiers produce a stronger and clearer signal.</li>
  <li>you can read as many times as you want from the amplifiers as the data is not being
deleted, <strong>burst mode</strong> is a single request to read multiple data that is adjacent 
in the same row. Less expensive then sending multiple requests through.</li>
  <li>When a new row needs to be read, the current row in the sense amplifiers has to be 
written back because the data was originally destroyed in the memory cells. This 
is an expensive operation as you have to write back and then read a new row. 
About 3 times more expensive then just reading a new column in the same row.</li>
</ol>

<p>The rate it takes to read and load data from the DRAM is dependant on the physical 
time it takes to charge and discharge capacitors.</p>

<p>Data read patterns matters because of the physics of the random access memory.</p>

<h2 id="so-what-does-this-all-mean">So what does this all mean?</h2>
<p>As the stride gets larger you start to read elements in different <strong>pages (rows)</strong> in 
the HBM. You have to constantly switch the capacitors on and off to load the pages 
creating a massive drop in memory bandwidth.</p>

<p>The memory access patterns are extrememly important because it can provide the biggest
boost in performance for GPUs. A 10 TFLOPs <strong>A100</strong> can be fed at rates of <strong>GFLOPs</strong>
instead with bad memory access patters, making the use of the A100 just pointeless at 
that point.</p>

<p>The reason we use GPUs is for performance and to get that performance we need to 
efficiently manage all of those GPU resources.</p>

<h2 id="inside-a-streaming-multiprocessor">Inside a Streaming Multiprocessor</h2>
<p>Each SM has a maximum memory bandwidth, that is why the GPU will load blocks into all 
the SMs to make sure that it is able to make the most of the full GPU memory bandwidth.</p>

<p>All blocks in the grid will run the same program using the same number of threads,
and they need 3 things:</p>
<ul>
  <li><strong>block size</strong>: the number of threads in each block</li>
  <li><strong>shared memory</strong>: high speed memory pool that all threads in the block can use</li>
  <li><strong>registers</strong>: working space of the thread, immiedately accessible by the thread for 
               operations. All threads will be running the same program so each 
               thread will have the same registers size (threds per block * 
               registers per thread = total registers count per block)</li>
</ul>

<h2 id="how-does-the-gpu-place-blocks-in-an-sm">How does the GPU place blocks in an SM?</h2>
<p>The GPU places blocks into SMs until one of the resources (threads, registers, shared
memory, or block limit) is exhausted. We call this <strong>occupancy</strong>.</p>

<p>Having multiple blocks per SM is crucial, because it enables latency hiding: when one
warp stalls on memory, another warp can be scheduled instantly. If only one block is
resident, latency hiding is severely limited, and performance suffers.</p>

<p>Therefore, after ensuring efficient memory access (e.g. coalesced reads/writes), the
next big lever is tuning occupancy, by adjusting block size, shared memory usage, and
register pressure to maximize throughput.</p>

<h2 id="what-can-we-do-with-the-gaps-in-the-sm">What can we do with the gaps in the SM?</h2>
<p>The gpu will try to run another grids blocks in the SMs as long as those new blocks 
resource requirements are lower than the remaining resources in the SM.</p>

<p>This is done with <strong>concurrency</strong>.</p>

<p>We can divide independent pieces of work into seperate <strong>streams</strong> and then the GPU 
can try and pack them into the SMs. It may not be able to start working on the blocks 
from different streams at the same time, due to resource limitations, but it can 
load a block from another stream causing the work timeline for that stream to be 
different than the original stream.</p>

<p>The SM will then just start going through all of the blocks it has in it and keep on 
loading new blocks and doing their computation once the resources are freed up, this
is the process of <strong>oversubscription</strong>.</p>

<h2 id="conclusion">Conclusion</h2>
<ol>
  <li>memory system can only feed a fraction of what the compute units can process</li>
  <li>the memory system itself depends on good access patterns, coalesced memory</li>
  <li>warp execution systems can counter this slow down by reading more than one thread 
at once. But we need all the threads in the warp to be reading memory adjacent to 
each other.</li>
  <li>the hardware spreads the blocks as widely as possible so all the SMs are being 
utilized, giving us a higher memory bandwidth.</li>
  <li>resource packing limitations have the next biggest imapct on performance</li>
  <li>to counter this we need to feed more data concurrently to the SM through multiple 
streams.</li>
  <li>CUDA works the way it does because of physics.</li>
</ol>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://www.youtube.com/watch?v=n6M8R8-PlnE&amp;t=1631s">How CUDA programming works (GTC 2022)</a></li>
  <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/contents.html">CUDA programming guide</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Ampere_(microarchitecture)">Ampere microarchitecutre</a></li>
  <li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">A100 whitepaper</a></li>
</ul>


        </div>
        <hr>
<footer>
    <div class="footer-container">
        <div class="footer-links-container"> 
            Find me here:
            <a href="https://github.com/ElPatatone">Github</a> |
            <a href="https://www.linkedin.com/in/hasanur-rahman-mohammad">Linkedin</a> |
            <a href="mailto:rahman.hm2002@gmail.com">Email</a>
        </div>
        <div class="quote-container">
            <p><em>
                "Real understanding begins where the abstraction ends."
            </em></p>
        </div>
    </div>
</footer>

    </div>
  </body>
</html>
